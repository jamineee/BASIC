# -*- coding: utf-8 -*-
"""app.py"""

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import json
from openai import OpenAI
import os
# CSV íŒŒì¼ ë¡œë“œ
file_path = '/content/drive/MyDrive/á„‹á…µá†«á„€á…©á†¼á„Œá…µá„‚á…³á†¼á„€á…µá„á…©/á„‹á…µá„Œá…¡á„†á…µá†«/full_petition_df.csv'
data = pd.read_csv(file_path)

#ì‚¬ì§„ íŒŒì¼ ë¡œë“œ
from PIL import Image

#PIL íŒ¨í‚¤ì§€ì— ì´ë¯¸ì§€ ëª¨ë“ˆì„ í†µí•´ ì´ë¯¸ì§€ ì—´ê¸° 
# Image.open('ì´ë¯¸ì§€ ê²½ë¡œ')
assembly_img = Image.open('/content/drive/MyDrive/á„‹á…µá†«á„€á…©á†¼á„Œá…µá„‚á…³á†¼á„€á…µá„á…©/á„‹á…µá„Œá…¡á„†á…µá†«/assembly_img.jpg')

col1,col2 = st.columns([2,3])
# ì œëª©
st.image(assembly_img)
st.write('ì‚¬ì§„ ì¶œì²˜: ëŒ€í•œë¯¼êµ­ êµ­íšŒ')
st.title("êµ­íšŒ êµ­ë¯¼ë™ì˜ ì²­ì› ëŒ€ì‹œë³´ë“œ")

# Tab ì„¤ì •
tab1, tab2 = st.tabs(['êµ­íšŒ êµ­ë¯¼ë™ì˜ ì²­ì› ì±—ë´‡', 'êµ­íšŒ êµ­ë¯¼ë™ì˜ ì²­ì› í˜„í™©'])

with tab1:
    # Tab A ë‚´ìš©
    #st.write('')
  
    os.environ["OPENAI_API_KEY"] = "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"  
    client = OpenAI()  
  
    st.title("ë‹¹ì‹ ì˜ ì²­ì› ì‘ì„±ì„ ë„ì™€ë“œë¦½ë‹ˆë‹¤!")  

    if 'messages' not in st.session_state:  
        st.session_state.messages = [{"role":"system","content":"ë‹¹ì‹ ì€ êµ­íšŒ êµ­ë¯¼ë™ì˜ ì²­ì›ì˜ ì£¼ì œë¥¼ ìš”ì•½í•˜ê³ , ì •í•´ì§„ 18ê°€ì§€ì˜ ë¶„ë¥˜(1.ì •ì¹˜/ì„ ê±°/êµ­íšŒìš´ì˜ 2.ìˆ˜ì‚¬/ë²•ë¬´/ì‚¬ë²•ì œë„ 3.ì¬ì •/ì„¸ì¬/ê¸ˆìœµ/ì˜ˆì‚° 4.ì†Œë¹„ì/ê³µì •ê±°ë˜ 5.êµìœ¡ 6.ê³¼í•™ê¸°ìˆ /ì •ë³´í†µì‹  7.ì™¸êµ/í†µì¼/êµ­ë°©/ì•ˆë³´ 8.ì¬ë‚œ/ì•ˆì „/í™˜ê²½ 9.í–‰ì •/ì§€ë°©ìì¹˜ 10.ë¬¸í™”/ì²´ìœ¡/ê´€ê´‘/ì–¸ë¡  11.ë†ì—…/ì„ì—…/ìˆ˜ì‚°ì—…/ì¶•ì‚°ì—… 12.ì‚°ì—…/í†µìƒ 13.ë³´ê±´ì˜ë£Œ 14.ë³µì§€/ë³´í›ˆ 15.êµ­í† /í•´ì–‘/êµí†µ 16.ì¸ê¶Œ/ì„±í‰ë“±/ë…¸ë™ 17.ì €ì¶œì‚°/ê³ ë ¹í™”/ì•„ë™/ì²­ì†Œë…„/ê°€ì¡± 18.ê¸°íƒ€)ì— ë”°ë¼ ì²­ì›ì„ ë¶„ë¥˜í•´ì£¼ëŠ” ì¸ê³µì§€ëŠ¥ì´ì•¼. ë˜í•œ ì£¼ì œì™€ ë¬¸ì œ ìƒí™©ì„ ì„¤ëª…í•´ì¤¬ì„ë•Œ, ì²­ì›ì„ 1000ì ì´ìƒìœ¼ë¡œ ì™„ì„±ì‹œì¼œì£¼ëŠ” ì¼ë„ ìˆ˜í–‰í•´ì•¼í•´. ì‚¬ìš©ìì˜ ìš”êµ¬ì— ë§ì¶° ì ì ˆí•œ ì‘ë‹µì„ ë‚´ì–´ì¤˜. ì²­ì› ìš”ì•½ì‹œì—ëŠ” ì²­ì› í…ìŠ¤íŠ¸ ë‚´ì—ì„œ ì£¼ì œì™€ ê´€ë ¨ëœ ìœ ì˜ë¯¸í•œ ë‹¨ì–´ë§Œ ì‚¬ìš©í•´ì„œ í…ìŠ¤íŠ¸ ë²„ë¸” ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ì–´ì¤˜. ì´ë•Œ ì‚¬ìš©ë˜ëŠ” ë‹¨ì–´ëŠ” 30ë‹¨ì–´ë¥¼ ë„˜ì§€ ë§ì•„. ë„ˆê°€ ì‘ì„±í•œ ì²­ì› ì „ë¬¸ì´ë‚˜, ì…ë ¥ëœ ì²­ì› ì „ë¬¸ì´ë“ , ì „ë¬¸ì´ ì¡´ì¬í•˜ê¸°ë§Œ í•œë‹¤ë©´ í…ìŠ¤íŠ¸ ë²„ë¸” ì´ë¯¸ì§€ë¥¼ í•­ìƒ ì¶œë ¥í•´ì¤˜."}
        ,{"role":"assistant","content":""}]  
  
    for msg in st.session_state.messages:
        if msg["role"] != "system":  # ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ì œì™¸
            if msg["role"] == "assistant":
                st.write(f"ğŸ¤– {msg['content']}")
            elif msg["role"] == "user":
                st.write(f"ğŸ§‘ {msg['content']}")

    user_message = st.text_input("User:", key="text_input1")  
    
    if st.button("Send"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        if user_message.strip():
            st.session_state.messages.append({"role": "user", "content": user_message})
        
        # OpenAI ì‘ë‹µ ìƒì„±
            completion = client.chat.completions.create(
                model="gpt-4o",
                messages=st.session_state.messages
        )
            response = completion.choices[0].message.content
            st.session_state.messages.append({"role": "assistant", "content": response})


# ë©”ì‹œì§€ ì¶œë ¥ (system ë©”ì‹œì§€ ì œì™¸, ì¤‘ë³µ ì¶œë ¥ ë°©ì§€)
    for msg in reversed(st.session_state.messages):
        if msg["role"] != "system":  # system ë©”ì‹œì§€ ì œì™¸
            with st.chat_message(msg['role']):
                st.write(msg['content'])

with tab2:
    # Tab B ë‚´ìš©
    st.write('êµ­íšŒ êµ­ë¯¼ë™ì˜ ì²­ì› ëŒ€ì‹œë³´ë“œ')
    col1, col2 = st.columns([2, 3])

    # Column 1
    with col1:
        st.subheader("ì£¼ì œë³„ ì²­ì› ë¶„ë¥˜")
        grouped_by_topic = data.groupby("ë¶„ì•¼").size()  # ì²­ì› ì£¼ì œë³„ ê±´ìˆ˜ ì§‘ê³„
        fig1, ax1 = plt.subplots()
        ax1.pie(
            grouped_by_topic, 
            labels=grouped_by_topic.index, 
            autopct='%1.1f%%', 
            startangle=140, 
            colors=plt.cm.Paired.colors
        )
        ax1.set_title("ì£¼ì œë³„ ì²­ì› ë¶„í¬")
        fig1.savefig('/content/topic.png')
        st.image('/content/topic.png')

    # Column 2
    with col2:
        st.subheader("ì²­ì› ë™ì˜ ìˆ˜")
        grouped_by_agreements = data.groupby("ë¶„ì•¼")["ë™ì˜ìˆ˜"].sum().sort_values(ascending=False)  # ì£¼ì œë³„ ë™ì˜ ìˆ˜ í•©ì‚°
        fig2, ax2 = plt.subplots()
        ax2.bar(
            grouped_by_agreements.index, 
            grouped_by_agreements.values, 
            color="skyblue"
        )
        ax2.set_title("ì²­ì› ì£¼ì œë³„ ë™ì˜ ìˆ˜")
        ax2.set_ylabel("ë™ì˜ ìˆ˜")
        ax2.set_xlabel("ì²­ì› ì£¼ì œ")
        plt.xticks(rotation=45)
        fig2.savefig('/content/nums.png')
        st.image('/content/nums.png')

import os
import re
import joblib
import streamlit as st
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# ì‚¬ìš©ì ì •ì˜ ì „ì²˜ë¦¬ í•¨ìˆ˜
def custom_preprocessor(text):
    text = text.lower()
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

with st.sidebar:
    st.title("ë‚´ ì²­ì›ì€ ì¤‘ìš”í• ê¹Œ?")
    st.write('ì²­ì› ì¤‘ìš”ë„ ê³„ì‚°ê¸°')
    st.write('ê³„ì‚°ê¸°ëŠ” ì§€ì—­, ì†Œìˆ˜ìì„± ë“± ë™ì˜ìˆ˜ ê¸°ì¤€ì˜ êµ­ë¯¼ ì²­ì›ì—ì„œ ë¬´ì‹œë  ìˆ˜ ìˆëŠ” ìš”ì†Œì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.')
    text_data = st.text_input("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", key="text_input2")

# íŒŒì¼ ê²½ë¡œ
    tokenizer1_path = "/content/drive/MyDrive/á„‹á…µá†«á„€á…©á†¼á„Œá…µá„‚á…³á†¼á„€á…µá„á…©/á„‹á…µá„Œá…¡á„†á…µá†«/local_weighted_tokenizer.pkl"
    tokenizer2_path = "/content/drive/MyDrive/á„‹á…µá†«á„€á…©á†¼á„Œá…µá„‚á…³á†¼á„€á…µá„á…©/á„‹á…µá„Œá…¡á„†á…µá†«/minority_weighted_tokenizer.pkl"
    model1_path = "/content/drive/MyDrive/á„‹á…µá†«á„€á…©á†¼á„Œá…µá„‚á…³á†¼á„€á…µá„á…©/á„‹á…µá„Œá…¡á„†á…µá†«/local_weighted_model.pkl"
    model2_path = "/content/drive/MyDrive/á„‹á…µá†«á„€á…©á†¼á„Œá…µá„‚á…³á†¼á„€á…µá„á…©/á„‹á…µá„Œá…¡á„†á…µá†«/minority_weighted_model.pkl"

# ì „ì²˜ë¦¬ê¸° ë° ëª¨ë¸ ë¡œë“œ
    try:
        tokenizer1 = AutoTokenizer.from_pretrained(tokenizer1_path)
        model1 = AutoModelForSequenceClassification.from_pretrained(model1_path)

        tokenizer2 = AutoTokenizer.from_pretrained(tokenizer2_path)
        model2 = AutoModelForSequenceClassification.from_pretrained(model2_path)
    except Exception as e:
        st.error(f"í† í¬ë‚˜ì´ì € ë˜ëŠ” ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    if text_data:
        try:
        # ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§• (Hugging Face ë°©ì‹)
            inputs1 = tokenizer1(text_data, return_tensors="pt", padding=True, truncation=True, max_length=512)
            inputs2 = tokenizer2(text_data, return_tensors="pt", padding=True, truncation=True, max_length=512)

        # ëª¨ë¸ ì˜ˆì¸¡
            with torch.no_grad():
                outputs1 = model1(**inputs1)
                outputs2 = model2(**inputs2)

        # ë¡œì§“ì„ ìŠ¤ì¹¼ë¼ë¡œ ë³€í™˜ (Softmaxë¥¼ ì‚¬ìš©í•˜ì—¬ í™•ë¥ ë¡œ ë³€í™˜ ê°€ëŠ¥)
            prediction1_scalar = torch.softmax(outputs1.logits, dim=1).max().item()
            prediction2_scalar = torch.softmax(outputs2.logits, dim=1).max().item()
        
            final_predict = max(prediction1_scalar, prediction2_scalar)

        # ê²°ê³¼ ì¶œë ¥
            if final_predict == prediction1_scalar:
                st.write(f"ì´ ì²­ì›ì€ ì§€ì—­ì„±ì„ ì§€ë‹™ë‹ˆë‹¤. ì§€ì—­ì„±ì—ì„œ ë‹¤ìŒì˜ ê°€ì¤‘ì¹˜ë¥¼ ì–»ìŠµë‹ˆë‹¤.: {prediction1_scalar:.2f}")
            else:
                st.write(f"ì´ ì²­ì›ì€ ì†Œìˆ˜ìì„±ì„ ì§€ë‹™ë‹ˆë‹¤. ì†Œìˆ˜ìì„±ì—ì„œ ë‹¤ìŒì˜ ê°€ì¤‘ì¹˜ë¥¼ ì–»ìŠµë‹ˆë‹¤.: {prediction2_scalar:.2f}")

        except Exception as e:
            st.error(f"ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    else:
        st.write("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")